{
  "monitors": {
    "_": [
      {
        "name": "node_high_cpu",
        "model": "threshold",
        "parameters": {
          "metric": "node_cpu_percent",
          "window": "2m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">=",
              "threshold": "99"
            }
          },
          "summary": "cpu saturated",
          "description": "cpu on {{$labels.instance}} over 99% busy"
        }
      },
      {
        "name": "node_memory_pressure",
        "model": "threshold",
        "parameters": {
          "metric": "node_memory_usable_percent",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": "<",
              "threshold": "5"
            }
          },
          "summary": "memory pressure",
          "description": "{{$labels.instance}} has low usable memory"
        }
      },
      {
        "name": "node_disk_filling",
        "model": "predict",
        "parameters": {
          "metric": "node_filesystem_free_percent",
          "period": "1h",
          "seconds": "28800",
          "window": "15m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": "<",
              "threshold": "0"
            }
          },
          "summary": "disk filling",
          "description": "{{$labels.instance}} will fill disk in {{$labels.prediction}}"
        }
      },
      {
        "name": "node_network_errors",
        "model": "threshold",
        "parameters": {
          "metric": "node_network_error_rate",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "summary": "network errors",
          "description": "{{$labels.instance}} has network errors"
        }
      },
      {
        "name": "kube_node_pressure",
        "model": "threshold",
        "parameters": {
          "metric": "kube_node_pressure",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "summary": "kube node pressure",
          "description": "{{$labels.node_name}} is in kubernetes condition {{$labels.condition}}"
        }
      },
      {
        "name": "restarts",
        "model": "threshold",
        "parameters": {
          "metric": "kube_pod_container_restarts_rate",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "summary": "container restarting rapidly",
          "description": "container {{$labels.container}} in pod {{$labels.pod}} of {{$labels.es_workload}} restarting rapidly"
        }
      },
      {
        "name": "kube_pod_not_ready",
        "model": "threshold",
        "parameters": {
          "metric": "kube_pod_ready",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "critical": {
              "comparator": "<",
              "threshold": "1"
            }
          },
          "summary": "pod not ready",
          "description": "pod {{$labels.pod}} on {{$labels.es_workload}} not ready"
        }
      },
      {
        "name": "kube_pod_not_running",
        "model": "threshold",
        "parameters": {
          "metric": "kube_pod_not_running",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "summary": "pod not running",
          "description": "pod {{$labels.pod}} on {{$labels.es_workload}} not running"
        }
      },
      {
        "name": "kube_workload_generation_lag",
        "model": "threshold",
        "parameters": {
          "metric": "kube_workload_generation_lag",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "summary": "es_workload not updating",
          "description": "es_workload {{$labels.es_workload}} has not updated for 10 minutes"
        }
      },
      {
        "name": "kafka_consumergroup_lag",
        "model": "growth",
        "parameters": {
          "metric": "kafka_consumergroup_lag",
          "period": "5m",
          "minslope": "1",
          "confidence": "1",
          "severity": {
            "warning": {
              "window": "5m"
            }
          },
          "summary": "consumergroup falling behind",
          "description": "{{$labels.consumergroup}} is falling behind"
        }
      },
      {
        "name": "scrape_time",
        "model": "sma",
        "parameters": {
          "metric": "scrape_duration_seconds",
          "period": "15m",
          "minval": "3",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "numsigma": "3"
            }
          },
          "summary": "scrape time anomalous",
          "description": "{{$labels.instance}} has anomalous scrape_duration_seconds"
        }
      },
      {
        "name": "akka_inbox_growth",
        "model": "growth",
        "parameters": {
          "metric": "akka_actor_mailbox_size{quantile=\"0.5\"}",
          "period": "15m",
          "minslope": "0.1",
          "confidence": "1",
          "severity": {
            "critical": {
              "window": "5m"
            }
          },
          "summary": "actor inbox growing",
          "description": "actor {{$labels.actor}} in {{$labels.app}} on {{$labels.instance}} has a growing inbox"
        }
      },
      {
        "name": "akka_processing_time",
        "model": "sma",
        "parameters": {
          "metric": "akka_actor_processing_time_ns{quantile=\"0.5\"}",
          "period": "15m",
          "minval": "100000000",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "summary": "actor processing time is anomalous",
          "description": "actor {{$labels.actor}} in {{$labels.app}} on {{$labels.instance}} has unusual processing time"
        }
      },
      {
        "name": "prometheus_notifications_dropped",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_notifications_dropped_rate",
          "window": "10m",
          "confidence": "0.25",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "summary": "Prometheus dropping notifications",
          "description": "Prometheus dropping alerts sent to Alertmanager"
        }
      },
      {
        "name": "prometheus_notification_queue",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_notification_queue_percent",
          "window": "10m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "50"
            }
          },
          "summary": "Prometheus alert queue filling",
          "description": "Prometheus alert queue is staying over 50% full"
        }
      },
      {
        "name": "prometheus_rule_evaluation_failures",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_rule_evaluation_failures_rate",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "summary": "Prometheus rule failures",
          "description": "Prometheus has {{$value}} rules failing"
        }
      },
      {
        "name": "prometheus_target_too_many_metrics",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_target_scrapes_exceeded_sample_limit_rate",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "summary": "Prometheus target over limit",
          "description": "Prometheus target at {{labels.instance}} has too many metrics"
        }
      },
      {
        "name": "prometheus_tsdb_reloads_failures",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_tsdb_reloads_failures_rate",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "critical": {
              "comparator": ">=",
              "threshold": "1"
            }
          },
          "summary": "Prometheus tsdb reload failing",
          "description": "Prometheus had {{$value}} reload failures"
        }
      },
      {
        "name": "prometheus_target_down",
        "model": "threshold",
        "parameters": {
          "metric": "up",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": "!=",
              "threshold": "1"
            }
          },
          "summary": "metrics target down",
          "description": "cannot connect to {{$labels.instance}} metrics endpoint for {{$labels.job}} data"
        }
      },
      {
        "name": "prometheus_config_reload_failed",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_config_last_reload_successful",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "critical": {
              "comparator": "!=",
              "threshold": "1"
            }
          },
          "summary": "prometheus bad config",
          "description": "current config for prometheus has errors, will prevent restarts"
        }
      },
      {
        "name": "prometheus_scrape_time",
        "model": "threshold",
        "parameters": {
          "metric": "prometheus_target_sync_percent",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "75"
            }
          },
          "summary": "prometheus has long scrape times",
          "description": "prometheus is taking {{$value}}% of the {{$labels.interval}} interval to get {{$labels.scrape_job}} metrics from {{$labels.instance}}"
        }
      },
      {
        "name": "zookeeper_latency",
        "model": "sma",
        "parameters": {
          "metric": "zk_avg_latency",
          "period": "15m",
          "minval": "10",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "summary": "Zookeeper Latency",
          "description": "ZooKeeper latency is not normal in {{$labels.es_workload}} on {{$labels.instance}}"
        }
      },
      {
        "name": "zookeeper_connections",
        "model": "sma",
        "parameters": {
          "metric": "zk_num_alive_connections",
          "period": "15m",
          "minval": "10",
          "window": "15m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "summary": "Zookeeper live connections is not normal",
          "description": "Zookeeper live connection in {{$labels.es_workload}} is not normal on {{$labels.instance}}"
        }
      },
      {
        "name": "zookeeper_pending_syncs",
        "model": "threshold",
        "parameters": {
          "metric": "zk_pending_syncs",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "critical": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "summary": "Zookeeper pending-syncs is not normal",
          "description": "Zookeeper Pending syncs in {{$labels.es_workload}} is greater than 0."
        }
      },
      {
        "name": "zookeeper_open_file_descriptor",
        "model": "sma",
        "parameters": {
          "metric": "zk_open_file_descriptor_count",
          "period": "15m",
          "minval": "10",
          "window": "15m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "summary": "Zookeeper open file descriptor growth",
          "description": "Zookeeper open file descriptors in ${{labels.es_workload}} is not normal in ${{labels.instance}}"
        }
      },
      {
        "name": "cassandra_write_latency",
        "model": "sma",
        "parameters": {
          "metric": "cassandra_clientrequest_write_latency",
          "period": "15m",
          "minval": "10",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "critical": {
              "numsigma": "2"
            }
          },
          "summary": "Cassandra Write Latency",
          "description": "Cassandra write latency is not normal on {{$labels.es_workload}} in {{$labels.instance}}"
        }
      },
      {
        "name": "cassandra_read_latency",
        "model": "sma",
        "parameters": {
          "metric": "cassandra_clientrequest_read_latency",
          "period": "15m",
          "minval": "10",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "critical": {
              "numsigma": "2"
            }
          },
          "summary": "Cassandra Read Latency",
          "description": "Cassandra read latency is not normal on {{$labels.es_workload}} in {{$labels.instance}}"
        }
      },
      {
        "name": "redis_keyspace_miss",
        "model": "sma",
        "parameters": {
          "metric": "redis_keyspace_miss_ratio",
          "period": "15m",
          "minval": "1",
          "window": "10m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "summary": "Redis key space miss ratio growth",
          "description": "Observing shifts in Redis key space ratio on ${{labels.es_workload}} in ${{labels.instance}}"
        }
      },
      {
        "name": "redis_evictions",
        "model": "sma",
        "parameters": {
          "metric": "redis_evicted_keys_total",
          "period": "15m",
          "minval": "1",
          "window": "10m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "summary": "Redis evictions growing",
          "description": "Redis evictions on {{$labels.es_workload}} are growing in {{$labels.instance}}"
        }
      },
      {
        "name": "redis_commands_processed",
        "model": "sma",
        "parameters": {
          "metric": "redis_commands_processed_total",
          "period": "10m",
          "minval": "10",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "3"
            }
          },
          "summary": "Redis command processed",
          "description": "Redis commands processed on {{$labels.es_workload}} is not normal in ${{labels.instance}}"
        }
      },
      {
        "name": "redis_connections",
        "model": "sma",
        "parameters": {
          "metric": "redis_connected_clients",
          "period": "15m",
          "minval": "10",
          "window": "15m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "summary": "Redis client connections is not normal",
          "description": "Redis client connections on {{$labels.es_workload}} is not normal in {{$labels.instance}}"
        }
      },
      {
        "name": "kafka_incoming_messages",
        "model": "sma",
        "parameters": {
          "metric": "kafka_incoming_messages_rate",
          "period": "5m",
          "minval": "10",
          "window": "10m",
          "confidence": "1",
          "severity": {
            "critical": {
              "numsigma": "2"
            }
          },
          "summary": "Kafka incoming message rate is not normal",
          "description": "Kafka incoming message rate on ${{labels.es_workload}} is not normal for topic ${{labels.topic}}"
        }
      },
      {
        "name": "kafka_offline_partition",
        "model": "threshold",
        "parameters": {
          "metric": "kafka_controller_kafkacontroller_offlinepartitionscount",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "critical": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "summary": "Kafka offline partition is not zero",
          "description": "Kafka offline partition is high on ${{labels.es_workload}} in ${{labels.instance}}"
        }
      },
      {
        "name": "kafka_under_replicated_partitions",
        "model": "threshold",
        "parameters": {
          "metric": "kafka_server_replicamanager_underreplicatedpartitions",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "critical": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "summary": "Kafka under replicated partitions is up",
          "description": "Kafka under replicated partitions is high on ${{labels.es_workload}} in ${{labels.instance}}"
        }
      },
      {
        "name": "memcached_miss_ratio",
        "model": "sma",
        "parameters": {
          "metric": "memcached_miss_ratio",
          "period": "15m",
          "minval": "10",
          "window": "15m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "summary": "Memcached miss ratio shifts",
          "description": "Memcached miss ratio in {{$labels.es_workload}} is not normal on {{$labels.instance}}"
        }
      },
      {
        "name": "memcached_current_connections",
        "model": "sma",
        "parameters": {
          "metric": "memcached_current_connections",
          "period": "15m",
          "minval": "10",
          "window": "15m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "summary": "Memcached connections are changing",
          "description": "Memcached connections in {{$labels.es_workload}} is not normal on {{$labels.instance}}"
        }
      },
      {
        "name": "memcached_evictions",
        "model": "sma",
        "parameters": {
          "metric": "memcached_evictions_rate",
          "period": "15m",
          "minval": "1",
          "window": "15m",
          "confidence": "1",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "summary": "Memcached evictions shifts",
          "description": "Memcached evictions in {{$labels.es_workload}} is not normal on {{$labels.instance}}"
        }
      },
      {
        "name": "akka_http_server_response_time",
        "model": "sma",
        "parameters": {
          "metric": "akka_http_http_server_response_time_ns{quantile=\"0.5\"}",
          "period": "15m",
          "minval": "100000000",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "summary": "HTTP server response time is anomalous",
          "description": "{{$labels.app}} on {{$labels.instance}} has unusual response time"
        }
      },
      {
        "name": "akka_http_client_response_time",
        "model": "sma",
        "parameters": {
          "metric": "akka_http_http_client_http_client_service_response_time_ns{quantile=\"0.5\"}",
          "period": "15m",
          "minval": "100000000",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "summary": "HTTP client response time is anomalous",
          "description": "{{$labels.app}} on {{$labels.instance}} has unusual response time"
        }
      },
      {
        "name": "akka_http_server_5xx",
        "model": "threshold",
        "parameters": {
          "metric": "akka_http_http_server_responses_5xx",
          "window": "5m",
          "confidence": "1",
          "severity": {
            "warning": {
              "comparator": ">",
              "threshold": "0"
            }
          },
          "summary": "HTTP 5xx errors",
          "description": "HTTP server at {{labels.instance}} has 5xx errors"
        }
      },
      {
        "name": "play_http_client_response_time",
        "model": "sma",
        "parameters": {
          "metric": "play_http_client_play_client_service_response_time_ns{quantile=\"0.5\"}",
          "period": "15m",
          "minval": "100000000",
          "window": "15m",
          "confidence": "0.5",
          "severity": {
            "warning": {
              "numsigma": "2"
            }
          },
          "summary": "HTTP client response time is anomalous",
          "description": "{{$labels.app}} on {{$labels.instance}} has unusual response time"
        }
      }
    ]
  }
}
